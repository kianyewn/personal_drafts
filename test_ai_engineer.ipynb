{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 5\n",
    "dim = 3\n",
    "query = torch.randn(1,L, dim)\n",
    "key = torch.randn(1,L, dim)\n",
    "value = torch.randn(1, L, dim)\n",
    "rel_pos_emb = torch.randn(2*L-1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [-1,  0,  1,  2,  3],\n",
       "        [-2, -1,  0,  1,  2],\n",
       "        [-3, -2, -1,  0,  1],\n",
       "        [-4, -3, -2, -1,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_pos = torch.arange(L)\n",
    "seq_pos[None, :] - seq_pos[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [-1,  0,  1,  2,  3],\n",
       "        [-2, -1,  0,  1,  2],\n",
       "        [-3, -2, -1,  0,  1],\n",
       "        [-4, -3, -2, -1,  0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos = seq_pos[:,None] - seq_pos[None,:] # (L, L)\n",
    "rel_pos = seq_pos[None, :] - seq_pos[:, None]\n",
    "rel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 6, 7, 8],\n",
       "        [3, 4, 5, 6, 7],\n",
       "        [2, 3, 4, 5, 6],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos2 = rel_pos + L -1 # convert all the negative indices to positive indices\n",
    "rel_pos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*L -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 6, 7, 8],\n",
       "        [3, 4, 5, 6, 7],\n",
       "        [2, 3, 4, 5, 6],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb_pos = rel_pos_emb[rel_pos2]  # (L, D)\n",
    "rel_pos_emb_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([9, 3]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos.shape, rel_pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 6, 7, 8],\n",
       "        [3, 4, 5, 6, 7],\n",
       "        [2, 3, 4, 5, 6],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0906,  0.3163, -1.4240],\n",
       "        [-1.0363,  0.7225,  0.9131],\n",
       "        [-0.9518,  0.1109, -0.5306],\n",
       "        [ 0.7835, -0.6042,  0.4329],\n",
       "        [ 0.0445,  0.7216, -1.0556],\n",
       "        [-0.1353,  0.7443,  1.8683],\n",
       "        [ 1.0703,  1.8171,  0.4764],\n",
       "        [-0.3758,  0.0659, -1.3575],\n",
       "        [ 1.4050, -0.8572,  0.4682]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 5, 6, 7, 8],\n",
       " [3, 4, 5, 6, 7],\n",
       " [2, 3, 4, 5, 6],\n",
       " [1, 2, 3, 4, 5],\n",
       " [0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5, 1]), torch.Size([9, 1, 3]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos2.unsqueeze(-1).shape, rel_pos_emb.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 3])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb_final = rel_pos_emb[rel_pos2] # (L, L, D)\n",
    "# query.matmul(rel_pos_emb_final.transpose(2,0).transpose(1,2).unsqueeze(0))\n",
    "rel_pos_emb_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B, 1, D), (T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1204, -1.2159,  0.3717, -0.0092,  2.0377]]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the attention score is correct\n",
    "query[:,0:1,:].matmul(rel_pos_emb[[4,5,6,7,8],:].unsqueeze(0).transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1204, -1.2159,  0.3717, -0.0092,  2.0377],\n",
       "         [-0.2955, -0.8731, -0.5363, -3.2205,  0.3245],\n",
       "         [-0.2409,  0.3046,  0.6400, -1.5759, -0.0343],\n",
       "         [-1.8551, -1.1569,  1.5205, -0.8030, -0.8787],\n",
       "         [ 3.9724, -2.9123,  0.5911, -0.3976,  2.3682]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relative attention score\n",
    "# (B,T,1,D), (1,T,D,T) -> (B, T, 1, T) -> (B, T, T)\n",
    "torch.matmul(query.unsqueeze(2), rel_pos_emb_final.unsqueeze(0).transpose(-1,-2)).squeeze(-2) # (B, T, T)\n",
    "# attention_scores = attention_scores + rel_pos_scores  # Add to attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*4*3 / (3*2*1) # Each unique group of 2 fruits has $2! = 2$ possible orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "0 1 3\n",
      "0 1 4\n",
      "0 2 3\n",
      "0 2 4\n",
      "0 3 4\n",
      "1 2 3\n",
      "1 2 4\n",
      "1 3 4\n",
      "2 3 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(5):\n",
    "    for j in range(i+1,5):\n",
    "        for k in range(j+1, 5):\n",
    "            print(i, j, k)\n",
    "            count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 3])\n",
      "torch.Size([1, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "# positional_embedding = torch.nn.Parameter(torch.randn(2 * L - 1, dim))\n",
    "positional_embedding = rel_pos_emb\n",
    "print(positional_embedding.shape) # torch.Size([9, 11])\n",
    "\n",
    "# X can be the Query or Key vectors of shape (B, T, D)\n",
    "relative_positional_encoding = torch.matmul(query, positional_embedding.transpose(0,1)) # B, T, 2*T-1)\n",
    "print(relative_positional_encoding.shape) # # torch.Size([2, 5, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2955, -0.8731, -0.5363, -3.2205,  0.3245, -0.7854])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_positional_encoding[0,1,-L-1:] # (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 5].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_pos_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1, 1, D) (T, D)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 5]."
     ]
    }
   ],
   "source": [
    "query[:,0:1,:].repeat(1,1,5,1).matmul(rel_pos_emb[[4,5,6,7,8],:].unsqueeze(0).unsqueeze(0)) # (B, 1, 1, D) (T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 3])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb_final.shape # (T, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1, 5])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(query.unsqueeze(2), rel_pos_emb_final.unsqueeze(0).transpose(-1,-2)).shape  # (B,T,D), (T,T, D) -> (batch_size, L, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1204, -1.2159,  0.3717, -0.0092,  2.0377],\n",
       "         [-0.2955, -0.8731, -0.5363, -3.2205,  0.3245],\n",
       "         [-0.2409,  0.3046,  0.6400, -1.5759, -0.0343],\n",
       "         [-1.8551, -1.1569,  1.5205, -0.8030, -0.8787],\n",
       "         [ 3.9724, -2.9123,  0.5911, -0.3976,  2.3682]]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(query.unsqueeze(2), rel_pos_emb_final.unsqueeze(0).transpose(-1,-2)).squeeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 3])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[:,0:1,:].repeat(1,1,5,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 3])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[:,0:1,:].repeat(1,1,5,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 3])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb[[4,5,6,7,8],:].unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B, T, D), (T,T,D) -> (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.6000, -1.1458, -0.1495],\n",
       "         [-1.6982, -2.9202,  0.0176],\n",
       "         [ 0.5109, -0.5625, -0.8548],\n",
       "         [-1.3728, -1.8783,  0.0779],\n",
       "         [-0.0455, -0.1973,  3.8646]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (B, T,1,D), (1, T, T,D) -> (B, T, T, D)\n",
    "(query.unsqueeze(2) * rel_pos_emb_final.unsqueeze(0)).sum(dim=-2) # (B,T, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1204, -1.2159,  0.3717, -0.0092,  2.0377]]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb[[4,5,6,7,8],:] # (T, D)\n",
    "query[:,0:1,:].matmul(rel_pos_emb[[4,5,6,7,8],:].unsqueeze(0).transpose(1,2)) # (B, 1, D), (1, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([1, 1, 3]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb[[4,5,6,7,8],:].shape, query[:,0:1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb[[4,5,6,7,8],:].unsqueeze(0).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[:,0:1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb[[4,5,6,7,8],:].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.batch_rel_pos_emb_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal of attention: (B,T,D) -> B, T, D\n",
    "# goal of rel_pos: similarity + rel_position_context # (B, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query @ rel_pos_emb_pos # (B, L, D), (L, L, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for r(i,j) = r(i-j+T-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 's') 5\n",
      "['T', 'h', 'is']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'H',\n",
       " 'u',\n",
       " 'g',\n",
       " 'g',\n",
       " 'in',\n",
       " 'g',\n",
       " 'F',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " 'C',\n",
       " 'ou',\n",
       " 'r',\n",
       " 'se',\n",
       " '.',\n",
       " 'tokeniz',\n",
       " 'er']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        word_freqs[token] += 1\n",
    "\n",
    "alphabets = []\n",
    "for word in word_freqs:\n",
    "    for letter in word:\n",
    "        if letter not in alphabets:\n",
    "            alphabets.append(letter)\n",
    "alphabets.sort()\n",
    "\n",
    "vocab = [ '<|endoftext|>'  ]+ alphabets.copy()\n",
    "\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)\n",
    "\n",
    "merges = {(\"i\", \"s\"): \"is\"}\n",
    "vocab.append(\"is\")\n",
    "\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "\n",
    "splits = merge_pair(\"i\", \"s\", splits)\n",
    "print(splits[\"This\"])\n",
    "\n",
    "\n",
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "    \n",
    "def tokenize(text):\n",
    "    pre_tokenized_text = text.split()\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n",
    "\n",
    "tokenize(\"This is the Hugging Face Course. tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('H', 'u'): 1,\n",
       "             ('u', 'g'): 1,\n",
       "             ('g', 'g'): 1,\n",
       "             ('g', 'in'): 1,\n",
       "             ('in', 'g'): 1,\n",
       "             ('F', 'a'): 1,\n",
       "             ('a', 'c'): 1,\n",
       "             ('c', 'e'): 1,\n",
       "             ('C', 'ou'): 1,\n",
       "             ('ou', 'r'): 1,\n",
       "             ('r', 'se'): 1,\n",
       "             ('se', '.'): 1,\n",
       "             ('c', 'h'): 1,\n",
       "             ('h', 'a'): 1,\n",
       "             ('a', 'p'): 1,\n",
       "             ('p', 't'): 1,\n",
       "             ('t', 'er'): 1,\n",
       "             ('ab', 'ou'): 1,\n",
       "             ('ou', 't'): 1,\n",
       "             ('tokeniz', 'at'): 1,\n",
       "             ('at', 'io'): 1,\n",
       "             ('io', 'n'): 2,\n",
       "             ('n', '.'): 1,\n",
       "             ('se', 'c'): 1,\n",
       "             ('c', 't'): 1,\n",
       "             ('t', 'io'): 1,\n",
       "             ('s', 'h'): 1,\n",
       "             ('h', 'o'): 2,\n",
       "             ('o', 'w'): 2,\n",
       "             ('w', 's'): 1,\n",
       "             ('se', 'v'): 1,\n",
       "             ('v', 'er'): 1,\n",
       "             ('er', 'a'): 1,\n",
       "             ('a', 'l'): 2,\n",
       "             ('tokeniz', 'er'): 1,\n",
       "             ('l', 'g'): 1,\n",
       "             ('g', 'o'): 1,\n",
       "             ('o', 'r'): 1,\n",
       "             ('r', 'i'): 1,\n",
       "             ('i', 'th'): 1,\n",
       "             ('th', 'm'): 1,\n",
       "             ('m', 's'): 1,\n",
       "             ('s', '.'): 2,\n",
       "             ('H', 'o'): 1,\n",
       "             ('o', 'p'): 1,\n",
       "             ('p', 'e'): 1,\n",
       "             ('e', 'f'): 1,\n",
       "             ('f', 'u'): 1,\n",
       "             ('u', 'l'): 1,\n",
       "             ('l', 'l'): 2,\n",
       "             ('l', 'y'): 1,\n",
       "             ('y', ','): 1,\n",
       "             ('y', 'ou'): 1,\n",
       "             ('w', 'i'): 1,\n",
       "             ('i', 'l'): 1,\n",
       "             ('b', 'e'): 1,\n",
       "             ('ab', 'l'): 1,\n",
       "             ('l', 'e'): 1,\n",
       "             ('u', 'nd'): 1,\n",
       "             ('nd', 'er'): 1,\n",
       "             ('er', 's'): 1,\n",
       "             ('s', 't'): 1,\n",
       "             ('t', 'a'): 1,\n",
       "             ('a', 'nd'): 2,\n",
       "             ('the', 'y'): 1,\n",
       "             ('a', 'r'): 1,\n",
       "             ('r', 'e'): 1,\n",
       "             ('t', 'r'): 1,\n",
       "             ('r', 'a'): 1,\n",
       "             ('a', 'in'): 1,\n",
       "             ('in', 'e'): 1,\n",
       "             ('e', 'd'): 1,\n",
       "             ('g', 'en'): 1,\n",
       "             ('en', 'er'): 1,\n",
       "             ('er', 'at'): 1,\n",
       "             ('at', 'e'): 1,\n",
       "             ('token', 's'): 1})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [ 1,2,3]\n",
    "l[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['This'],\n",
       " 'is': ['is'],\n",
       " 'the': ['the'],\n",
       " 'Hugging': ['H', 'u', 'g', 'g', 'in', 'g'],\n",
       " 'Face': ['F', 'a', 'c', 'e'],\n",
       " 'Course.': ['C', 'ou', 'r', 'se', '.'],\n",
       " 'chapter': ['c', 'h', 'a', 'p', 't', 'er'],\n",
       " 'about': ['ab', 'ou', 't'],\n",
       " 'tokenization.': ['tokeniz', 'at', 'ion', '.'],\n",
       " 'section': ['se', 'c', 't', 'ion'],\n",
       " 'shows': ['s', 'h', 'o', 'w', 's'],\n",
       " 'several': ['se', 'v', 'er', 'a', 'l'],\n",
       " 'tokenizer': ['tokeniz', 'er'],\n",
       " 'algorithms.': ['a', 'l', 'g', 'o', 'r', 'i', 'th', 'm', 's', '.'],\n",
       " 'Hopefully,': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y', ','],\n",
       " 'you': ['y', 'ou'],\n",
       " 'will': ['w', 'i', 'l', 'l'],\n",
       " 'be': ['b', 'e'],\n",
       " 'able': ['ab', 'l', 'e'],\n",
       " 'to': ['to'],\n",
       " 'understand': ['u', 'nd', 'er', 's', 't', 'a', 'nd'],\n",
       " 'how': ['h', 'o', 'w'],\n",
       " 'they': ['the', 'y'],\n",
       " 'are': ['a', 'r', 'e'],\n",
       " 'trained': ['t', 'r', 'a', 'in', 'e', 'd'],\n",
       " 'and': ['a', 'nd'],\n",
       " 'generate': ['g', 'en', 'er', 'at', 'e'],\n",
       " 'tokens.': ['token', 's', '.']}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('i', 's'): 'is',\n",
       " ('e', 'r'): 'er',\n",
       " ('t', 'o'): 'to',\n",
       " ('e', 'n'): 'en',\n",
       " ('T', 'h'): 'Th',\n",
       " ('Th', 'is'): 'This',\n",
       " ('t', 'h'): 'th',\n",
       " ('o', 'u'): 'ou',\n",
       " ('s', 'e'): 'se',\n",
       " ('to', 'k'): 'tok',\n",
       " ('tok', 'en'): 'token',\n",
       " ('n', 'd'): 'nd',\n",
       " ('th', 'e'): 'the',\n",
       " ('i', 'n'): 'in',\n",
       " ('a', 'b'): 'ab',\n",
       " ('token', 'i'): 'tokeni',\n",
       " ('tokeni', 'z'): 'tokeniz',\n",
       " ('a', 't'): 'at',\n",
       " ('i', 'o'): 'io',\n",
       " ('io', 'n'): 'ion'}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i', 's')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
